# data_loader.py

## 훈련용(train)과 테스트용(test)의 전처리를 다르게 하는 이유?
- 훈련용 데이터에는 **데이터 다양화**를 적용하여 모델의 일반화 성능을 높이고자 한다.
- 테스트용 데이터는 **모델의 성능 평가 기준**이므로, 원본 이미지에 가깝게 유지해야 하므로
  → 정규화만 적용한다.

### 즉,
- `훈련용(train)`: 모델이 다양한 데이터를 보면서 **덜 민감하게** 학습되도록 일부러 흔들어 줌.
- `테스트용(test)`: 진짜 성능을 평가하기 위해 **변형 없이** 깔끔하게 넣어줌

## transforms.Compose([...]) 전처리 설명
- `RandomCrop(32, padding=4)`: 4픽셀 padding 후 무작위로 32x32 잘라서 모델이 위치 변화에 덜 민감하게 학습되도록 함.
- `RandomHorizontalFlip()`: 이미지 좌우 반전(50% 확률) → 대칭 변화에 강건한 모델 학습
- `ToTensor()`: `PIL.Image`나 `numpy.ndarray`를 PyTorch의 `[0, 1]` 범위의 Tensor로 변환
- `Normalize(mean, std)`: 각 채널(R, G, B)을 **평균 0, 표준편차 1** 분포로 바꿔서 더 빠르고 안정적인 학습을 유도

## 실험 조건 바꾼 이유와 기대 효과

- 전처리: `ColorJitter`와 `RandomRotation`을 통해 **데이터 다양성을 인위적으로 증가**시켜 일반화 성능을 향상 시키고자 함 → 모델이 **다양한 시각적 변형에도 강인하게 반응**하도록 학습, **과적합 방지** 및 테스트 정확도 향상 기대

- Conv Layer 추가: 기존보다 더 깊은 모델을 구성하여 **이미지의 복잡한 특징(Edge → Shape → Object 등)을 더 잘 추출**하고자 함 → 특징 표현력 강화, 더 정교한 특성 맵 학습 가능

- Dropout 적용: 학습 중 일부 뉴런을 임의로 제거하여 **특정 뉴런에 과도하게 의존하는 현상**을 막기 위해 사용 → 훈련 데이터에 덜 민감한 일반화된 모델 학습

- Optimizer 및 learning Rate 변경(Adam → SGD, 0.001 → 0.01): **Adam은 빠르게 수렴**하지만, 일반화 성능이 떨어질 수 있음, 반면에 **SGD는 느리지만 안정적**이고 일반화 성능이 좋은 경향이 있다. **학습 속도를 맞추기** 위해 learning rate를 0.001로 조정 → 더 느리지만 **안정적인 수렴, 과적합 완화 기대** 
    
# 실험 결과
1. 기본적인 전처리 과정을 거치고 CNN 모델을 학습하였을 때

1회 학습: **56.27%** → 10회 학습: **73.71%**

2. 전처리 과정을 추가하였을 때 //

1회 학습: **78.93%** → 10회 학습: **79.02**

3. CNN 구조 변화

   1) Conv 레이어 1개 추가 // 

      1회 학습: **58.77%** → 10회 학습: **78.47%**

   2) Drop out 있는 모델 // 
  
      1회 학습: **56.65%** → 10회 학습: **74.35%**
  
4. Optimizer 와 Learning Rate 변화

1회 학습: **35.82%** → 10회 학습: **62.75%**

## 분석
- **전처리 추가**한 것이 가장 큰 성능 향상을 보인다.
- **Conv 레이어를 추가**한 것 또한, 좋은 성능 향상을 가져온다.
- **Dropout**은 약간의 향상
- **Optimizer**와 **Learning Rate** 변화는 오히려 성능을 낮추는 결과가 발생 